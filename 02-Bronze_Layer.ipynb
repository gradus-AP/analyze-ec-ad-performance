{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffdb756",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Databricks Notebook\n",
    "# ===================================\n",
    "# 02_Bronze_Layer\n",
    "# ===================================\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# -------------------------\n",
    "# 入力データ（Volumes）\n",
    "# -------------------------\n",
    "VOLUME_PATH = \"/Volumes/azure_databricks_test/default/sample_data\"\n",
    "\n",
    "# -------------------------\n",
    "# 出力（Unity Catalog）\n",
    "# -------------------------\n",
    "CATALOG_NAME = \"azure_databricks_test\"   # ←環境に合わせて変更可\n",
    "SCHEMA_NAME  = \"ad_analytics\"            # ←あなたのスキーマ名\n",
    "DB_3PART     = f\"{CATALOG_NAME}.{SCHEMA_NAME}\"\n",
    "\n",
    "# カタログ／スキーマ準備（存在しなければ作成）\n",
    "spark.sql(f\"USE CATALOG {CATALOG_NAME}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {DB_3PART}\")\n",
    "\n",
    "print(\"=\"*68)\n",
    "print(\"ブロンズレイヤー構築開始 (catalog.schema = \" + DB_3PART + \")\")\n",
    "print(\"=\"*68)\n",
    "\n",
    "# ============================================================\n",
    "# 1. 商品マスタ（JSON）: トップレベルが配列 → 展開して読み込み\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n[1/3] 商品マスタ（JSON配列）取り込み中...\")\n",
    "\n",
    "# ① そのまま JSON を読み込む（今の構造に合う）\n",
    "items_parsed_df = spark.read.option(\"multiline\", \"true\").json(f\"{VOLUME_PATH}/items/*.json\")\n",
    "\n",
    "# ② メタ列付与\n",
    "items_bronze_df = items_parsed_df \\\n",
    "    .withColumn(\"ingestion_timestamp\", current_timestamp()) \\\n",
    "    .withColumn(\"source_system\", lit(\"product_master_db\")) \\\n",
    "    .withColumn(\"file_name\", lit(f\"{VOLUME_PATH}/items\"))\n",
    "\n",
    "# ③ 保存（Unity Catalog マネージドテーブル）\n",
    "items_bronze_df.write \\\n",
    "    .format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(f\"{DB_3PART}.bronze_items\")\n",
    "\n",
    "print(f\"✓ 商品マスタ取り込み完了: {items_bronze_df.count():,}件\")\n",
    "\n",
    "# ============================================================\n",
    "# 2. 広告データ（CSV）\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n[2/3] 広告データ（CSV）取り込み中...\")\n",
    "\n",
    "ads_schema = StructType([\n",
    "    StructField(\"campaign_id\", StringType(), True),\n",
    "    StructField(\"campaign_name\", StringType(), True),\n",
    "    StructField(\"ad_id\", StringType(), True),\n",
    "    StructField(\"ad_platform\", StringType(), True),\n",
    "    StructField(\"ad_format\", StringType(), True),\n",
    "    StructField(\"target_url\", StringType(), True),\n",
    "    StructField(\"impressions\", IntegerType(), True),\n",
    "    StructField(\"clicks\", IntegerType(), True),\n",
    "    StructField(\"cost\", DoubleType(), True),\n",
    "    StructField(\"date\", StringType(), True),          # ブロンズでは文字列のまま\n",
    "    StructField(\"utm_source\", StringType(), True),\n",
    "    StructField(\"utm_medium\", StringType(), True),\n",
    "    StructField(\"utm_campaign\", StringType(), True),\n",
    "    StructField(\"target_category\", StringType(), True)\n",
    "])\n",
    "\n",
    "ads_raw_df = spark.read.schema(ads_schema) \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(f\"{VOLUME_PATH}/digital_ads/*-ads.csv\")\n",
    "\n",
    "ads_bronze_df = ads_raw_df \\\n",
    "    .withColumn(\"ingestion_timestamp\", current_timestamp()) \\\n",
    "    .withColumn(\"source_system\", lit(\"google_ads_api\")) \\\n",
    "    .withColumn(\"file_name\", lit(f\"{VOLUME_PATH}/digital_ads\"))\n",
    "\n",
    "ads_bronze_df.write \\\n",
    "    .format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(f\"{DB_3PART}.bronze_digital_ads\")\n",
    "\n",
    "print(f\"✓ 広告データ取り込み完了: {ads_bronze_df.count():,}件\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. トランザクションデータ（CSV）\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n[3/3] トランザクションデータ（CSV）取り込み中...\")\n",
    "\n",
    "transactions_schema = StructType([\n",
    "    StructField(\"transaction_id\", StringType(), True),\n",
    "    StructField(\"transaction_timestamp\", StringType(), True),  # ブロンズでは文字列\n",
    "    StructField(\"item_id\", StringType(), True),\n",
    "    StructField(\"user_email\", StringType(), True),\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"price\", IntegerType(), True),\n",
    "    StructField(\"referrer_url\", StringType(), True),\n",
    "    StructField(\"landing_page_url\", StringType(), True),\n",
    "    StructField(\"utm_source\", StringType(), True),\n",
    "    StructField(\"utm_medium\", StringType(), True),\n",
    "    StructField(\"utm_campaign\", StringType(), True),\n",
    "    StructField(\"device_type\", StringType(), True),\n",
    "    StructField(\"session_id\", StringType(), True),\n",
    "    StructField(\"conversion_time_minutes\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "transactions_raw_df = spark.read.schema(transactions_schema) \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(f\"{VOLUME_PATH}/transactions/*-transactions.csv\")\n",
    "\n",
    "transactions_bronze_df = transactions_raw_df \\\n",
    "    .withColumn(\"ingestion_timestamp\", current_timestamp()) \\\n",
    "    .withColumn(\"source_system\", lit(\"ec_site_db\")) \\\n",
    "    .withColumn(\"file_name\", lit(f\"{VOLUME_PATH}/transactions\"))\n",
    "\n",
    "transactions_bronze_df.write \\\n",
    "    .format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(f\"{DB_3PART}.bronze_transactions\")\n",
    "\n",
    "print(f\"✓ トランザクション取り込み完了: {transactions_bronze_df.count():,}件\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 完了確認（SHOW TABLES / 先頭表示 / 総件数）\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*68)\n",
    "print(\"ブロンズレイヤー構築完了\")\n",
    "print(\"=\"*68)\n",
    "\n",
    "print(\"\\n【テーブル一覧 (SHOW TABLES)】\")\n",
    "display(spark.sql(f\"SHOW TABLES IN {DB_3PART}\"))\n",
    "\n",
    "print(\"\\n【テーブル先頭表示 & 総件数】\")\n",
    "for t in [\"bronze_items\", \"bronze_digital_ads\", \"bronze_transactions\"]:\n",
    "    print(f\"\\n== {DB_3PART}.{t} ==\")\n",
    "    display(spark.table(f\"{DB_3PART}.{t}\").limit(5))\n",
    "    cnt = spark.table(f\"{DB_3PART}.{t}\").count()\n",
    "    print(f\"record count: {cnt}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
